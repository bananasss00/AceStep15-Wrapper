import sys
import os
import tempfile
import torch
import torchaudio
import folder_paths
import hashlib
import json
import shutil
import glob
import comfy.utils
import comfy.model_management as mm
import gc

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

from safetensors.torch import load_file, save_file
from acestep.handler import AceStepHandler
from acestep.llm_inference import LLMHandler
from acestep.inference import generate_music, GenerationParams, GenerationConfig, format_sample

# === –ù–ê–°–¢–†–û–ô–ö–ê –ü–£–¢–ï–ô ===
ACESTEP_MODELS_DIR = os.path.join(folder_paths.models_dir, "acestep")
if not os.path.exists(ACESTEP_MODELS_DIR):
    os.makedirs(ACESTEP_MODELS_DIR)

GLOBAL_ACESTEP_HANDLERS = []

def cleanup_all_acestep():
    global GLOBAL_ACESTEP_HANDLERS
    if not GLOBAL_ACESTEP_HANDLERS:
        return
    
    print("\n[ACE-Step] –°–∏—Å—Ç–µ–º–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ ComfyUI. –í—ã–≥—Ä—É–∑–∫–∞ ACE-Step –∏–∑ VRAM...\n")
    for dit_handler, llm_handler in GLOBAL_ACESTEP_HANDLERS:
        try:
            if llm_handler:
                llm_handler.unload()
        except: pass
        try:
            if dit_handler:
                dit_handler.model = None
                dit_handler.vae = None
                dit_handler.text_encoder = None
                dit_handler._base_decoder = None
        except: pass
    
    GLOBAL_ACESTEP_HANDLERS.clear()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
    gc.collect()
    print("[ACE-Step] –ü–∞–º—è—Ç—å —É—Å–ø–µ—à–Ω–æ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∞.")

if not hasattr(mm, "_original_unload_all_models_acestep"):
    mm._original_unload_all_models_acestep = mm.unload_all_models

    def hooked_unload_all_models(*args, **kwargs):
        cleanup_all_acestep()
        return mm._original_unload_all_models_acestep(*args, **kwargs)
    
    mm.unload_all_models = hooked_unload_all_models

# ============================================================================
# 1. –ó–∞–≥—Ä—É–∑—á–∏–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏
# ============================================================================
class AceStepModelLoader:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "config_path": (["acestep-v15-turbo", "acestep-v15-base", "acestep-v15-sft"], {"default": "acestep-v15-turbo"}),
                "device": (["auto", "cuda", "mps", "cpu"], {"default": "auto"}),
                "init_llm": ("BOOLEAN", {"default": True, "label_on": "Yes", "label_off": "No"}),
                "lm_model_path": (["acestep-5Hz-lm-1.7B", "acestep-5Hz-lm-0.6B", "acestep-5Hz-lm-4B"], {"default": "acestep-5Hz-lm-1.7B"}),
                "lm_backend": (["vllm", "pt", "mlx"], {"default": "vllm"}),
                "use_flash_attention": ("BOOLEAN", {"default": True}),
                "offload_to_cpu": ("BOOLEAN", {"default": False}),
            }
        }

    RETURN_TYPES = ("ACESTEP_MODEL",)
    RETURN_NAMES = ("model",)
    FUNCTION = "load_model"
    CATEGORY = "ACE-Step"

    def load_model(self, config_path, device, init_llm, lm_model_path, lm_backend, use_flash_attention, offload_to_cpu):
        print(f"[ACE-Step] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è. –¶–µ–ª–µ–≤–∞—è –ø–∞–ø–∫–∞ –º–æ–¥–µ–ª–µ–π: {ACESTEP_MODELS_DIR}")
        
        cleanup_all_acestep()
        
        dit_handler = AceStepHandler()
        llm_handler = LLMHandler()

        dit_handler._get_project_root = lambda: ACESTEP_MODELS_DIR
        project_root = ACESTEP_MODELS_DIR

        status, enable_gen = dit_handler.initialize_service(
            project_root=project_root,
            config_path=config_path,
            device=device,
            use_flash_attention=use_flash_attention,
            compile_model=False,
            offload_to_cpu=offload_to_cpu,
            offload_dit_to_cpu=offload_to_cpu
        )

        if not enable_gen:
            raise RuntimeError(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ DiT –º–æ–¥–µ–ª–∏: {status}")

        if init_llm:
            print(f"[ACE-Step] –ó–∞–≥—Ä—É–∑–∫–∞ LLM {lm_model_path}...")
            checkpoint_dir = os.path.join(project_root, "checkpoints")
            
            from acestep.model_downloader import ensure_lm_model
            try:
                ensure_lm_model(model_name=lm_model_path, checkpoints_dir=checkpoint_dir)
            except Exception as e:
                print(f"[ACE-Step] –û—à–∏–±–∫–∞ –∞–≤—Ç–æ-—Å–∫–∞—á–∏–≤–∞–Ω–∏—è LLM: {e}")

            lm_status, lm_success = llm_handler.initialize(
                checkpoint_dir=checkpoint_dir,
                lm_model_path=lm_model_path,
                backend=lm_backend,
                device=device,
                offload_to_cpu=offload_to_cpu,
            )
            if not lm_success:
                print(f"[ACE-Step] Warning LLM: {lm_status}")

        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —Ö–µ–Ω–¥–ª–µ—Ä—ã –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–º –º–∞—Å—Å–∏–≤–µ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –æ—á–∏—Å—Ç–∫–∏
        GLOBAL_ACESTEP_HANDLERS.append((dit_handler, llm_handler if llm_handler.llm_initialized else None))

        return ({"dit_handler": dit_handler, "llm_handler": llm_handler if llm_handler.llm_initialized else None, "active_adapters": {}},)

# ============================================================================
# 2. –ó–∞–≥—Ä—É–∑—á–∏–∫ LoRA
# ============================================================================
class AceStepLoraLoader:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("ACESTEP_MODEL",),
                "lora_path": ("STRING", {"default": "", "multiline": False, "placeholder": "–ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ LoRA"}),
                "strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 4.0, "step": 0.05}),
                "enable_lora": ("BOOLEAN", {"default": True}),
                "ignore_bias_error": ("BOOLEAN", {"default": True, "tooltip": "–§–∏–ª—å—Ç—Ä—É–µ—Ç bias –∏–∑ –≤–µ—Å–æ–≤ –≤ –ø–∞–º—è—Ç–∏"}),
            },
            "optional": {
                 "adapter_name_override": ("STRING", {"default": "", "multiline": False}),
            }
        }

    RETURN_TYPES = ("ACESTEP_MODEL",)
    RETURN_NAMES = ("model",)
    FUNCTION = "load_lora"
    CATEGORY = "ACE-Step"

    def load_lora(self, model, lora_path, strength, enable_lora, ignore_bias_error, adapter_name_override=""):
        if not enable_lora or not lora_path.strip():
            return (model,)

        dit_handler = model["dit_handler"]
        final_path = lora_path.strip()
        
        if not os.path.exists(final_path):
             print(f"[ACE-Step] Warning: LoRA path not found: {final_path}")
             return (model,)

        suffix = "_nb" if ignore_bias_error else ""
        if adapter_name_override.strip():
            adapter_name = adapter_name_override.strip() + suffix
        else:
            path_hash = hashlib.md5(final_path.encode()).hexdigest()[:8]
            base_name = os.path.basename(final_path.rstrip(os.sep)).split('.')[0]
            adapter_name = f"{base_name}_{path_hash}{suffix}"

        load_msg = dit_handler.add_lora(final_path, adapter_name=adapter_name, ignore_bias=ignore_bias_error)

        if "‚ùå" in load_msg and "already loaded" not in load_msg:
            print(f"[ACE-Step] –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ LoRA {adapter_name}: {load_msg}")
        else:
            new_active = model["active_adapters"].copy()
            new_active[adapter_name] = float(strength)
            new_model = model.copy()
            new_model["active_adapters"] = new_active
            return (new_model,)

        return (model,)
    
# ============================================================================
# –ó–∞–ø–µ–∫–∞–Ω–∏–µ (Merge) LoRA –≤ –º–æ–¥–µ–ª—å
# ============================================================================
class AceStepLoraBaker:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("ACESTEP_MODEL",),
                "output_dir": ("STRING", {"default": folder_paths.get_output_directory(), "multiline": False}),
                "file_name": ("STRING", {"default": "acestep_merged_model.safetensors", "multiline": False}),
            }
        }

    RETURN_TYPES = ("ACESTEP_MODEL", "STRING")
    RETURN_NAMES = ("model", "saved_path")
    FUNCTION = "bake_loras"
    CATEGORY = "ACE-Step"

    def load_lora_sd(self, path):
        """–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ—Å–æ–≤ LoRA"""
        if os.path.isdir(path):
            st_path = os.path.join(path, "adapter_model.safetensors")
            bin_path = os.path.join(path, "adapter_model.bin")
            if os.path.exists(st_path): 
                return load_file(st_path)
            elif os.path.exists(bin_path): 
                return torch.load(bin_path, map_location="cpu", weights_only=True)
        else:
            if path.endswith('.safetensors'): 
                return load_file(path)
            else: 
                return torch.load(path, map_location="cpu", weights_only=True)
        return None

    def bake_loras(self, model, output_dir, file_name):
        dit_handler = model["dit_handler"]
        active_adapters = model.get("active_adapters", {})
        
        if not active_adapters:
            print("[ACE-Step Baker] ‚ö†Ô∏è –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö LoRA –¥–ª—è –∑–∞–ø–µ–∫–∞–Ω–∏—è. –ü–æ–¥–∫–ª—é—á–∏—Ç–µ AceStepLoraLoader.")
            return (model, "")
            
        os.makedirs(output_dir, exist_ok=True)
        save_path = os.path.join(output_dir, file_name)
        
        print("\n[ACE-Step Baker] üç≥ –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª–Ω–æ–π –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å –¥–∏—Å–∫–∞...")
        
        # === –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–æ–ª—É—á–∞–µ–º –∏–º—è –º–æ–¥–µ–ª–∏ –∏–∑ last_init_params ===
        config_path = None
        if hasattr(dit_handler, "last_init_params") and dit_handler.last_init_params:
            config_path = dit_handler.last_init_params.get("config_path")
        
        # –ï—Å–ª–∏ –≤–¥—Ä—É–≥ –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–±—É–µ–º —Ö–∞—Ä–¥–∫–æ–¥ (–æ–±—ã—á–Ω–æ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è)
        if not config_path:
            config_path = "acestep-v15-turbo" # Fallback
            print(f"[ACE-Step Baker] ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏–º—è –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º {config_path}")

        base_model_dir = os.path.join(dit_handler._get_project_root(), "checkpoints", config_path)
        base_model_path = os.path.join(base_model_dir, "model.safetensors")
        
        if not os.path.exists(base_model_path):
            raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω –±–∞–∑–æ–≤—ã–π —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏: {base_model_path}")
            
        print(f"[ACE-Step Baker] –ß—Ç–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞: {base_model_path}")
        merged_sd = load_file(base_model_path)
        
        registry = getattr(dit_handler, "_lora_service", None).registry if hasattr(dit_handler, "_lora_service") else {}
        
        lora_prefix = "base_model.model."
        base_prefix = "decoder."

        for adapter_name, strength in active_adapters.items():
            if strength == 0.0:
                continue
                
            lora_path = registry.get(adapter_name, {}).get("path", "")
            if not lora_path or not os.path.exists(lora_path):
                print(f"[ACE-Step Baker] ‚ùå –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ –Ω–∞–π–¥–µ–Ω –ø—É—Ç—å –¥–ª—è –∞–¥–∞–ø—Ç–µ—Ä–∞ {adapter_name}")
                continue
                
            print(f"[ACE-Step Baker] üíâ –ó–∞–ø–µ–∫–∞–Ω–∏–µ LoRA: {adapter_name} (Strength: {strength})")
            lora_sd = self.load_lora_sd(lora_path)
            
            if lora_sd is None:
                print(f"[ACE-Step Baker] ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤–µ—Å–∞ –¥–ª—è {adapter_name}")
                continue
            
            peft_config = dit_handler.model.decoder.peft_config.get(adapter_name)
            if peft_config:
                lora_r = peft_config.r
                lora_alpha = peft_config.lora_alpha
            else:
                lora_r, lora_alpha = 64, 64 
                
            scale = (lora_alpha / lora_r) * strength
            
            merged_count = 0
            for lora_key, lora_tensor in lora_sd.items():
                if "lora_A" in lora_key:
                    # LoRA key: base_model.model.layers.0...
                    # Target key: decoder.layers.0...
                    base_key = lora_key.replace(lora_prefix, base_prefix).replace(".lora_A.", ".")
                    lora_b_key = lora_key.replace("lora_A", "lora_B")
                    
                    if base_key in merged_sd and lora_b_key in lora_sd:
                        W = merged_sd[base_key]
                        A = lora_tensor
                        B = lora_sd[lora_b_key]
                        
                        orig_dtype = W.dtype
                        
                        W_f32 = W.to(torch.float32)
                        A_f32 = A.to(torch.float32)
                        B_f32 = B.to(torch.float32)
                        
                        if W.ndim == 2 and A.ndim == 2 and B.ndim == 2:
                            delta_W = B_f32 @ A_f32
                            W_f32 += delta_W * scale
                            merged_sd[base_key] = W_f32.to(orig_dtype)
                            merged_count += 1
                            
                elif "lora_B" in lora_key:
                    continue
                    
                else:
                    # Bias –∏ –¥—Ä—É–≥–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    base_key = lora_key.replace(lora_prefix, base_prefix)
                        
                    if base_key in merged_sd:
                        W = merged_sd[base_key]
                        orig_dtype = W.dtype
                        
                        W_f32 = W.to(torch.float32)
                        L_f32 = lora_tensor.to(torch.float32)
                        
                        W_f32 = W_f32 + (L_f32 - W_f32) * strength
                        merged_sd[base_key] = W_f32.to(orig_dtype)
                        merged_count += 1

            print(f"[ACE-Step Baker] –£—Å–ø–µ—à–Ω–æ —Å–ª–∏—Ç–æ –º–∞—Ç—Ä–∏—Ü: {merged_count}")

        print(f"[ACE-Step Baker] üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –≤ {save_path} ...")
        save_file(merged_sd, save_path)
        print(f"[ACE-Step Baker] ‚ú® –ó–∞–ø–µ–∫–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {os.path.getsize(save_path) / (1024**3):.2f} –ì–ë")
        
        return (model, save_path)

# ============================================================================
# 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ LLM
# ============================================================================
class AceStepLMConfig:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "lm_temperature": ("FLOAT", {"default": 0.85, "min": 0.0, "max": 2.0, "step": 0.05}),
                "lm_cfg_scale": ("FLOAT", {"default": 2.0, "min": 1.0, "max": 5.0, "step": 0.1}),
                "lm_top_p": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 1.0, "step": 0.05}),
                "lm_top_k": ("INT", {"default": 0, "min": 0, "max": 100, "step": 1}),
                "audio_cover_strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.05, "tooltip": "–°–∏–ª–∞ –∞—É–¥–∏–æ-–∫–æ–¥–æ–≤ / Cover strength"}),
                "lm_negative_prompt": ("STRING", {"default": "NO USER INPUT", "multiline": True}),
                "use_cot_metas": ("BOOLEAN", {"default": True, "tooltip": "–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (BPM, —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å) —á–µ—Ä–µ–∑ LLM"}),
                "use_cot_caption": ("BOOLEAN", {"default": True, "tooltip": "–†–∞–∑—Ä–µ—à–∏—Ç—å LLM –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—Ç—å caption –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–µ—Å–ª–∏ –Ω–µ –∑–∞–ø—Ä–µ—â–µ–Ω–æ –≤ —Å–∞–º–æ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–µ)"}),
                "use_cot_language": ("BOOLEAN", {"default": True, "tooltip": "–û–ø—Ä–µ–¥–µ–ª—è—Ç—å —è–∑—ã–∫ —á–µ—Ä–µ–∑ LLM"}),
                "allow_lm_batch": ("BOOLEAN", {"default": True, "tooltip": "–†–∞–∑—Ä–µ—à–∏—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤ LLM"}),
            }
        }

    RETURN_TYPES = ("ACESTEP_LM_CONFIG",)
    RETURN_NAMES = ("lm_config",)
    FUNCTION = "create_config"
    CATEGORY = "ACE-Step"

    def create_config(self, lm_temperature, lm_cfg_scale, lm_top_p, lm_top_k, audio_cover_strength, 
                      lm_negative_prompt, use_cot_metas, use_cot_caption, use_cot_language, allow_lm_batch):
        return ({
            "lm_temperature": lm_temperature,
            "lm_cfg_scale": lm_cfg_scale,
            "lm_top_p": lm_top_p,
            "lm_top_k": lm_top_k,
            "audio_cover_strength": audio_cover_strength,
            "lm_negative_prompt": lm_negative_prompt,
            "use_cot_metas": use_cot_metas,
            "use_cot_caption": use_cot_caption,
            "use_cot_language": use_cot_language,
            "allow_lm_batch": allow_lm_batch
        },)

# ============================================================================
# 4. –£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ —á–µ—Ä–µ–∑ LLM
# ============================================================================
class AceStepPromptEnhancer:
    """
    –ü—Ä–æ–≥–æ–Ω—è–µ—Ç –±–∞–∑–æ–≤—ã–µ caption –∏ lyrics —á–µ—Ä–µ–∑ LLM (format_sample).
    –ü–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–ª–∏ –∑–∞–¥–∞—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤—Ä—É—á–Ω—É—é.
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("ACESTEP_MODEL",),
                "caption": ("STRING", {"multiline": True, "default": "pop song"}),
                "lyrics": ("STRING", {"multiline": True, "default": ""}),
                "keep_orig_caption": ("BOOLEAN", {"default": False, "tooltip": "–û—Ç–¥–∞—Ç—å –Ω–∞ –≤—ã—Ö–æ–¥ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π Caption, –∏–≥–Ω–æ—Ä–∏—Ä—É—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç LLM"}),
                "keep_orig_lyrics": ("BOOLEAN", {"default": False, "tooltip": "–û—Ç–¥–∞—Ç—å –Ω–∞ –≤—ã—Ö–æ–¥ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ Lyrics, –∏–≥–Ω–æ—Ä–∏—Ä—É—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç LLM"}),
            },
            "optional": {
                "lm_config": ("ACESTEP_LM_CONFIG",),
                "bpm_override": ("INT", {"default": 0, "min": 0, "max": 300, "tooltip": "0 = –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LLM"}),
                "keyscale_override": ("STRING", {"default": "", "tooltip": "–ü—É—Å—Ç–æ = –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LLM"}),
                "time_signature_override": ("STRING", {"default": "", "tooltip": "–ü—É—Å—Ç–æ = –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LLM"}),
                "language_override": (["none", "unknown", "en", "zh", "ja", "ru", "es", "fr", "de"], {"default": "none"}),
            }
        }

    RETURN_TYPES = ("STRING", "STRING", "INT", "STRING", "STRING", "STRING")
    RETURN_NAMES = ("enhanced_caption", "enhanced_lyrics", "bpm", "key_scale", "time_signature", "vocal_language")
    FUNCTION = "enhance"
    CATEGORY = "ACE-Step"

    def enhance(self, model, caption, lyrics, keep_orig_caption, keep_orig_lyrics,
                lm_config=None, bpm_override=0, keyscale_override="", 
                time_signature_override="", language_override="none"):
        
        llm_handler = model.get("llm_handler")
        
        # –ï—Å–ª–∏ LLM –Ω–µ—Ç, –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—ã –∏–ª–∏ –æ–≤–µ—Ä—Ä–∞–π–¥—ã
        if not llm_handler or not getattr(llm_handler, "llm_initialized", False):
            print("[ACE-Step Enhancer] Warning: LLM –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ.")
            return (
                caption, 
                lyrics, 
                bpm_override, 
                keyscale_override.strip(), 
                time_signature_override.strip(), 
                language_override if language_override != "none" else "unknown"
            )

        temp = 0.85
        top_k = None
        top_p = None

        if lm_config:
            temp = lm_config.get("lm_temperature", 0.85)
            tk = lm_config.get("lm_top_k", 0)
            tp = lm_config.get("lm_top_p", 0.9)
            top_k = tk if tk > 0 else None
            top_p = tp if tp < 1.0 else None

        print("[ACE-Step Enhancer] –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ LLM...")
        result = format_sample(
            llm_handler=llm_handler,
            caption=caption,
            lyrics=lyrics,
            user_metadata=None,
            temperature=temp,
            top_k=top_k,
            top_p=top_p,
            use_constrained_decoding=True,
        )

        if not result.success:
            print(f"[ACE-Step Enhancer] –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–ª—É—á—à–µ–Ω–∏–∏ –ø—Ä–æ–º–ø—Ç–∞: {result.error or result.status_message}")
            return (caption, lyrics, bpm_override, keyscale_override, time_signature_override, language_override)

        # –í—ã–±–∏—Ä–∞–µ–º, —á—Ç–æ –æ—Ç–¥–∞—Ç—å –Ω–∞ –≤—ã—Ö–æ–¥: —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ LLM –∏–ª–∏ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ —é–∑–µ—Ä–æ–º
        final_caption = caption if keep_orig_caption else (result.caption or caption)
        final_lyrics = lyrics if keep_orig_lyrics else (result.lyrics or lyrics)
        final_bpm = bpm_override if bpm_override > 0 else (result.bpm or 0)
        final_key = keyscale_override.strip() if keyscale_override.strip() else (result.keyscale or "")
        final_ts = time_signature_override.strip() if time_signature_override.strip() else (result.timesignature or "")
        final_lang = language_override if language_override != "none" else (result.language or "unknown")

        return (final_caption, final_lyrics, final_bpm, final_key, final_ts, final_lang)

# ============================================================================
# 5. –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ú—É–∑—ã–∫–∏
# ============================================================================
class AceStepMusicGenerator:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("ACESTEP_MODEL",),
                "task_type": (["text2music", "cover", "repaint"], {"default": "text2music"}),
                "caption": ("STRING", {"multiline": True, "default": "piano solo"}),
                "lyrics": ("STRING", {"multiline": True, "default": "[Instrumental]"}),
                "duration": ("FLOAT", {"default": -1.0, "min": -1.0, "max": 600.0}),
                "inference_steps": ("INT", {"default": 8}),
                "guidance_scale": ("FLOAT", {"default": 7.0}),
                "shift": ("FLOAT", {"default": 3.0, "min": 1.0, "max": 5.0, "step": 0.1, "tooltip": "–°–º–µ—â–µ–Ω–∏–µ —Ç–∞–π–º—Å—Ç–µ–ø–æ–≤ (Timestep shift factor)"}),
                "thinking": ("BOOLEAN", {"default": True, "tooltip": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ-–∫–æ–¥–æ–≤ —á–µ—Ä–µ–∑ LLM"}),
                "seed": ("INT", {"default": -1, "min": -1, "max": 0xffffffffffffffff}),
                "unload_unused_loras": ("BOOLEAN", {"default": True}),
            },
            "optional": {
                "reference_audio": ("AUDIO",),
                "source_audio": ("AUDIO",),
                "vocal_language": (["unknown", "en", "zh", "ja", "ru"], {"default": "unknown"}),
                "bpm": ("INT", {"default": 0}),
                "key_scale": ("STRING", {"default": ""}),
                "time_signature": ("STRING", {"default": ""}),
                "lm_config": ("ACESTEP_LM_CONFIG",),
            }
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio",)
    FUNCTION = "generate"
    CATEGORY = "ACE-Step"

    def _save_comfy_audio_to_temp(self, comfy_audio) -> str:
        if comfy_audio is None: return None
        waveform = comfy_audio["waveform"].squeeze(0)
        sample_rate = comfy_audio["sample_rate"]
        fd, temp_path = tempfile.mkstemp(suffix=".wav")
        os.close(fd)
        torchaudio.save(temp_path, waveform, sample_rate)
        return temp_path

    def _sync_loras(self, dit_handler, requested_adapters, unload_unused):
        """
        –†—É—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ—è–º–∏ PEFT –¥–ª—è –æ–±—Ö–æ–¥–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π ACE-Step –∏ —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π PEFT.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç add_weighted_adapter –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ —Å–º–µ—à–∏–≤–∞–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö LoRA.
        """
        if not dit_handler.lora_loaded:
            return

        loaded_adapters = list(dit_handler._active_loras.keys())
        decoder = getattr(dit_handler.model, "decoder", None)
        combo_name = "comfy_mixed_lora"
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã—Ö –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ (–±–∞–π–ø–∞—Å—Å –≤—Å–µ—Ö –Ω–æ–¥)
        if not requested_adapters:
            if unload_unused:
                print("[ACE-Step] –ó–∞–ø—Ä–æ—à–µ–Ω–Ω—ã—Ö LoRA –Ω–µ—Ç. –ü–æ–ª–Ω–∞—è –≤—ã–≥—Ä—É–∑–∫–∞ PEFT...")
                dit_handler.unload_lora()
            else:
                print("[ACE-Step] –ó–∞–ø—Ä–æ—à–µ–Ω–Ω—ã—Ö LoRA –Ω–µ—Ç. –û—Ç–∫–ª—é—á–∞–µ–º —Å–ª–æ–∏...")
                if decoder and hasattr(decoder, "disable_adapter_layers"):
                    decoder.disable_adapter_layers()
                dit_handler.use_lora = False
            return

        active_names = []
        active_weights = []
        
        for name in loaded_adapters:
            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –Ω–∞—à –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∞–¥–∞–ø—Ç–µ—Ä –¥–ª—è —Å–º–µ—à–∏–≤–∞–Ω–∏—è –ø—Ä–∏ –ø–µ—Ä–µ–±–æ—Ä–µ
            if name == combo_name:
                continue

            if name in requested_adapters:
                weight = float(requested_adapters[name])
                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤–µ—Å –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ —Å—Ç–µ–π—Ç–∞ ACE-Step
                dit_handler._active_loras[name] = weight
                
                if weight > 0.0:
                    active_names.append(name)
                    active_weights.append(weight)
            else:
                if unload_unused:
                    print(f"[ACE-Step] –í—ã–≥—Ä—É–∑–∫–∞ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–π LoRA: {name}")
                    try:
                        dit_handler.remove_lora(name)
                    except Exception:
                        print(f"[ACE-Step] –°–±—Ä–æ—Å –≤—Å–µ—Ö LoRA –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –≤—ã–≥—Ä—É–∑–∫–∏.")
                        dit_handler.unload_lora()
                        return
                else:
                    dit_handler._active_loras[name] = 0.0

        # –ê–∫—Ç–∏–≤–∞—Ü–∏—è –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ (–û–±—Ö–æ–¥ –æ—à–∏–±–∫–∏ TypeError: unhashable type: 'list')
        if decoder is not None:
            if active_names:
                if hasattr(decoder, "enable_adapter_layers"):
                    decoder.enable_adapter_layers()
                
                # –ï—Å–ª–∏ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ, —Å–º–µ—à–∏–≤–∞–µ–º –∏—Ö –≤ –æ–¥–∏–Ω –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π
                if len(active_names) > 1 and hasattr(decoder, "add_weighted_adapter"):
                    try:
                        if combo_name in decoder.peft_config:
                            decoder.delete_adapter(combo_name)
                        
                        # –õ–∏–Ω–µ–π–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤–µ—Å–æ–≤
                        decoder.add_weighted_adapter(
                            adapters=active_names, 
                            weights=active_weights, 
                            adapter_name=combo_name, 
                            combination_type="linear"
                        )
                        decoder.set_adapter(combo_name)
                        print(f"[ACE-Step] –°–º–µ—à–∞–Ω—ã –º—É–ª—å—Ç–∏-LoRA: {active_names} —Å –≤–µ—Å–∞–º–∏ {active_weights}")
                    except Exception as e:
                        print(f"[ACE-Step] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–º–µ—à–∞—Ç—å –∞–¥–∞–ø—Ç–µ—Ä—ã: {e}. –í–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π: {active_names[0]}")
                        decoder.set_adapter(active_names[0])
                else:
                    # –ï—Å–ª–∏ –∞–¥–∞–ø—Ç–µ—Ä –æ–¥–∏–Ω, –ø—Ä–æ—Å—Ç–æ –∞–∫—Ç–∏–≤–∏—Ä—É–µ–º –µ–≥–æ
                    adapter_name = active_names[0]
                    try:
                        decoder.set_adapter(active_names)
                    except TypeError:
                        # Fallback –¥–ª—è —Å—Ç–∞—Ä—ã—Ö PEFT
                        decoder.set_adapter(adapter_name)
                    
                    try:
                        dit_handler.set_lora_scale(adapter_name, active_weights[0])
                        # print(f"[ACE-Step] –ü—Ä–∏–º–µ–Ω–µ–Ω –≤–µ—Å {active_weights[0]} –¥–ª—è LoRA '{adapter_name}'")
                    except Exception as e:
                        print(f"[ACE-Step] –û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤–µ—Å–∞ LoRA: {e}")
                        
                dit_handler.use_lora = True
                dit_handler.lora_scale = active_weights[0] if active_weights else 1.0
            else:
                if hasattr(decoder, "disable_adapter_layers"):
                    decoder.disable_adapter_layers()
                dit_handler.use_lora = False

    def generate(self, model, task_type, caption, lyrics, duration, inference_steps, 
                 guidance_scale, shift, thinking, seed, unload_unused_loras, 
                 reference_audio=None, source_audio=None, vocal_language="unknown", 
                 bpm=0, key_scale="", time_signature="", lm_config=None):
        
        dit_handler = model["dit_handler"]
        llm_handler = model["llm_handler"]
        active_adapters = model.get("active_adapters", {})

        self._sync_loras(dit_handler, active_adapters, unload_unused_loras)

        if thinking and llm_handler is None:
            thinking = False

        ref_path = self._save_comfy_audio_to_temp(reference_audio)
        src_path = self._save_comfy_audio_to_temp(source_audio)

        # –í—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã LLM (–≤–∫–ª—é—á–∞—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã CoT)
        lm_temp = 0.85
        lm_cfg = 2.0
        lm_tk = 0
        lm_tp = 0.9
        cover_str = 1.0
        lm_neg_prompt = "NO USER INPUT"
        use_cot_metas = True
        use_cot_caption = True
        use_cot_language = True
        allow_lm_batch = True

        if lm_config:
            lm_temp = lm_config.get("lm_temperature", 0.85)
            lm_cfg = lm_config.get("lm_cfg_scale", 2.0)
            lm_tk = lm_config.get("lm_top_k", 0)
            lm_tp = lm_config.get("lm_top_p", 0.9)
            cover_str = lm_config.get("audio_cover_strength", 1.0)
            lm_neg_prompt = lm_config.get("lm_negative_prompt", "NO USER INPUT")
            use_cot_metas = lm_config.get("use_cot_metas", True)
            use_cot_caption = lm_config.get("use_cot_caption", True)
            use_cot_language = lm_config.get("use_cot_language", True)
            allow_lm_batch = lm_config.get("allow_lm_batch", True)

        params = GenerationParams(
            task_type=task_type, caption=caption, lyrics=lyrics,
            bpm=bpm if bpm > 0 else None, keyscale=key_scale, timesignature=time_signature,
            duration=duration, vocal_language=vocal_language,
            inference_steps=inference_steps, guidance_scale=guidance_scale,
            shift=shift, seed=seed,
            thinking=thinking, reference_audio=ref_path,
            src_audio=src_path if task_type != "text2music" else None,
            use_cot_metas=use_cot_metas, 
            use_cot_caption=use_cot_caption, 
            use_cot_language=use_cot_language,
            audio_cover_strength=cover_str,
            lm_temperature=lm_temp, lm_cfg_scale=lm_cfg, lm_top_k=lm_tk, 
            lm_top_p=lm_tp, lm_negative_prompt=lm_neg_prompt
        )

        config = GenerationConfig(
            batch_size=1, 
            use_random_seed=(seed == -1), 
            audio_format="wav",
            allow_lm_batch=allow_lm_batch
        )

        # ==========================================
        # –ü–†–û–ì–†–ï–°–° –ë–ê–† COMFYUI
        # ==========================================
        pbar = comfy.utils.ProgressBar(100)
        last_percent = 0

        def progress_callback(value, desc=None, *args, **kwargs):
            nonlocal last_percent
            if isinstance(value, str):
                # print(f"[ACE-Step] {value}")
                return
                
            if isinstance(value, (int, float)):
                current_percent = min(100, max(0, int(value * 100)))
                if current_percent > last_percent:
                    pbar.update(current_percent - last_percent)
                    last_percent = current_percent
                    
            if desc:
                pass # print(f"[ACE-Step Progress] {desc} ({last_percent}%)")

        print("[ACE-Step] –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é...")
        try:
            result = generate_music(
                dit_handler=dit_handler, 
                llm_handler=llm_handler, 
                params=params, 
                config=config, 
                save_dir=None,
                progress=progress_callback
            )
            
            if not result.success:
                raise RuntimeError(f"Generation Failed: {result.error}")

            if last_percent < 100:
                pbar.update(100 - last_percent)

            output_audio_tensor = result.audios[0]["tensor"]
            output_sample_rate = result.audios[0]["sample_rate"]

            return ({"waveform": output_audio_tensor.unsqueeze(0), "sample_rate": output_sample_rate},)

        finally:
            if ref_path and os.path.exists(ref_path): os.remove(ref_path)
            if src_path and os.path.exists(src_path): os.remove(src_path)

NODE_CLASS_MAPPINGS = {
    "AceStepModelLoader": AceStepModelLoader,
    "AceStepLoraLoader": AceStepLoraLoader,
    "AceStepLoraBaker": AceStepLoraBaker,
    "AceStepLMConfig": AceStepLMConfig,
    "AceStepPromptEnhancer": AceStepPromptEnhancer,
    "AceStepMusicGenerator": AceStepMusicGenerator
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "AceStepModelLoader": "ACE-Step Model Loader üéµ",
    "AceStepLoraLoader": "ACE-Step LoRA Loader üíä",
    "AceStepLoraBaker": "ACE-Step LoRA Baker üç≥",
    "AceStepLMConfig": "ACE-Step LM Config ‚öôÔ∏è",
    "AceStepPromptEnhancer": "ACE-Step Prompt Enhancer ‚úçÔ∏è",
    "AceStepMusicGenerator": "ACE-Step Music Generator üéµ"
}